---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---

#Word Embeded Models

This notebook will use word embeded models to explore the corpus of railroad law.Given the relatively small size and narrow focus of the corpus, I am going to test the usefulness of word embeded models on such a corpus.
```{r libraries, results="hide"}
library(text2vec)
library(magrittr)
library(wordVectors)
library(tsne)
library(ggplot2)
library(tibble)
```

```{r}
#prep_word2vec("C:/Users/Joshua/Documents/rdata/railroaddata/railroads_documents","rr_docs.txt",lowercase=T) 

```

```{r}
# rr_model = train_word2vec("rr_docs.txt",output="rr_doc_vectors.bin",threads = 6,vectors = 100,window=12)
```
Since I created the vector space model of railroad law previously using the code above, I simply read in this model.This saves an great deal of time as the creation of the model took well over an hour for 397 documents. 
```{r loading, results="hide"}
rr_model = read.vectors("rr_doc_vectors.bin")
```
###Similarity Searching
The first method of exploration I used was similarity searching.To demonstrate one challenge to using this method, I chose the word "buffalo." As shown below, this returned a list of mostly cities, but I was interested in the animal. Therefore, I needed to perform some addition and subtraction on my word vectors.
```{r}
rr_model %>% nearest_to(rr_model[["buffalo"]])
```


Attempting to subtract the cities from the list did not return my desired outcome. 
```{r}
rr_model %>% nearest_to(rr_model[["buffalo"]] - rr_model[[c("ontario","rochester")]])

```
Attempting to add "cattle" removed the cities but did not yield very interesting results. 
```{r}
rr_model %>% nearest_to(rr_model[["buffalo"]] - rr_model[[c("ontario","rochester")]] + rr_model[["cattle"]])
```
Therefore, I looked at the similarity between "buffalo" and "cattle" and found that they are not very similar (.0098 cosine similarity).
```{r}
cosineSimilarity(rr_model[[c("buffalo"),average=F]], rr_model[[c("cattle"), average=F]])

```
I needed to use a different approach. Below I used the reject function to attempt to filter out unwanted definitions/meanings/semantics of "buffalo." As the results show, this was also not effective. "Buffalo" the animal is not used enough in the context of an animal in this corpus. 
```{r}
not_that_kind_of_buffalo = rr_model[["buffalo"]] %>%
      reject(rr_model[["city"]]) %>% 
      reject(rr_model[["rochester"]]) %>%   
      reject(rr_model[["ontario"]])%>%   
      reject(rr_model[["utica"]]) 

rr_model %>% nearest_to(not_that_kind_of_buffalo)
```

Next, I looked at another example.
Looking at the word "sheep" revealed that the animals included in the corpus are types of cargo. 
```{r}
rr_model %>% nearest_to(rr_model[["sheep"]], 20)
```
Interestingly, wool is absent from this above list. Similarly, "sheep" is absent from the similarity search for "wool."

```{r}
rr_model %>% nearest_to(rr_model[["wool"]], 20)
```


Despite the fact that wool and sheep seem unrelated based upon the similarity searching, they do have a fairly high cosine similarity score (0.38). 
```{r}
cosineSimilarity(rr_model[[c("sheep"),average=F]], rr_model[[c("wool"), average=F]])
```
The same is true for hides/cows (0.35).
```{r}
cosineSimilarity(rr_model[[c("cows"),average=F]], rr_model[[c("hides"), average=F]])

```

Thus, the classic king - man + women = queen example should work for sheep and cows (wool - sheep + cow = hide). However, this is not the case. The results a a jumble of names, perhaps defendents or prosecutors of the cases. 

```{r}
rr_model %>% nearest_to(rr_model[["wool"]] - rr_model[[c("sheep")]] + rr_model[["cow"]], 25)
```
I then tried reversing the terms and that improved the output. This time it returned a collction of animals that may have been skinned if not necessarily turned into a hide. Still cattle is one of the results. Thus, I concluded that there are limitations to the application of this method of word embeded modeling. 
```{r}
rr_model %>% nearest_to(rr_model[["sheep"]] - rr_model[[c("wool")]] + rr_model[["hide"]])
```

###Plotting Characteristics

Below I experimented with the informaiton gathered in the examples above. One thing that I noticed was that "sheep" was part of a group of animals (but not "buffalo") and that "wool" was part of a list of commodities. So, I further explored these different types of cargo (living/nonliving) and further divided them as perishable/non perishable. 

```{r}
rr_model %>% nearest_to(rr_model[["eggs"]], 50)

perishable = nearest_to(rr_model, rr_model[[c("eggs", "butter", "vegetables", "asparagus", "cheese", "lettuce", "potatoes", "peas", "margarine", "oranges", "beans", "beef", "lemons", "beeries", "cabbage", "peaches", "peaches", "apples")]], 150)

non_perishable = nearest_to(rr_model, rr_model[[c("cotton", "lumber", "cement", "coal", "iron", "steel", "shingles", "lime", "turpentine", "petroleum", "fertilizer", "salt", "leather")]], 150)

living_cargo = nearest_to(rr_model, rr_model[[c("sheep", "swine", "hogs", "goats", "cattle", "cows", "pigs", "mules", "calves", "poultry")]], 150)

non_living_cargo = nearest_to(rr_model, rr_model[[c("wool", "hides", "meat", "cheese", "grain", "potatoes", "hay", "cotton", "four", "leather", "hops", "fruit", "vegetables", "sugar", "canned", "eggs", "stuffs", "dried", "groceries", "canned")]], 150)
                     
lifelike = rr_model[[names(living_cargo), average =F]]
lifeless = rr_model[[names(non_living_cargo), average =F]]

cargo_words = c("eggs", "butter", "vegetables", "asparagus", "cheese", "lettuce", "potatoes", "peas", "margarine", "oranges", "beans", "beef", "lemons", "beeries", "cabbage", "peaches", "peaches", "apples", "cotton", "lumber", "cement", "coal", "iron", "steel", "shingles", "lime", "turpentine", "petroleum", "fertilizer", "salt", "leather", "wool", "hides", "meat", "cheese", "grain", "potatoes", "hay", "cotton", "flour", "leather", "hops", "fruit", "vegetables", "sugar", "canned", "eggs", "stuffs", "dried", "groceries", "canned", "sheep", "swine", "hogs", "goats", "cattle", "cows", "pigs", "mules", "calves", "poultry")

cargo = rr_model[rownames(rr_model) %in% cargo_words, ]

perishable_score = cargo %>% cosineSimilarity(rr_model[[c("eggs", "butter", "vegetables", "asparagus", "cheese", "lettuce", "potatoes", "peas", "margarine", "oranges", "beans", "beef", "lemons", "beeries", "cabbage", "peaches", "peaches", "apples")]])

non_perishable_score = cargo %>% cosineSimilarity(rr_model[[c("cotton", "lumber", "cement", "coal", "iron", "steel", "shingles", "lime", "turpentine", "petroleum", "fertilizer", "salt", "leather")]])

non_living_score = cargo %>% cosineSimilarity(rr_model[[c("wool", "hides", "meat", "cheese", "grain", "potatoes", "hay", "cotton", "four", "leather", "hops", "fruit", "vegetables", "sugar", "canned", "eggs", "stuffs", "dried", "groceries", "canned")]])
                                              
living_score = cargo %>% cosineSimilarity(rr_model[[c("sheep", "swine", "hogs", "goats", "cattle", "cows", "pigs", "mules", "calves", "poultry")]]) 

dfpvnp <- data_frame(x = perishable_score[, 1], y = non_perishable_score[, 1], labels = rownames(cargo))

dflvnl <- data_frame(x = living_score[, 1], y = non_living_score[, 1], labels = rownames(cargo))

dflvnp <-data_frame(x = living_score[, 1], y = non_perishable_score[, 1], labels = rownames(cargo))

#plot(perishable_score,non_perishable_score,type='n',main="Top 300 cargo words plotted by their similarity to perishable\n(x axis) and non perishable (y axis).")
#text(perishable_score,non_perishable_score,labels=rownames(cargo),cex=.7) 
#abline(a=0,b=1)

#some_fish = nearest_to(model,model[[c("fish","salmon","trout","shad","flounder","carp","roe","eels")]],150)
#fishy = model[[names(some_fish),average=F]]
```
Below is a plot of perishable(ness). The plot does a decent job of seperating the two groups; however, the model still gave lumber a ver high non perishable score and a relatively high perishable score. How can it be both? Similarly animals which are perishable do not score high on either score. Therefore, it can be infered that time is not the variable that differentiates between the groups. Still the clusters of words say something, but what is unclear.
```{r}
ggplot (dfpvnp, aes(x, y, label = labels)) + geom_text() + geom_point() + coord_equal() + xlab("Perishable Score") + ylab("Non Perishable Score") + ggtitle("Perishable(ness)") + geom_abline(intercept = 0, slope = 1)

```

Below is a plot of alive(ness). Here the model did pretty well at locating the living animals. It also seperated organic and inorganic materials. 
```{r}
ggplot (dflvnl, aes(x, y, label = labels)) + geom_text() + geom_point() + coord_equal() + xlab("Living Score") + ylab("Not Living Score") + ggtitle("Alive(ness)") 
```
Below is a chart that combines the two sets of variables (living vs non perishable). Again the model does a good job of identifying living animals. It is even a bit unsure of where to place poultry which could refer to a shipment of living or dead chickens. 
```{r}

ggplot (dflvnp, aes(x, y, label = labels)) + geom_text() + geom_point() + coord_equal() + xlab("Living Score") + ylab("Not Perishable") + ggtitle("Alive but Not Perishable?") 

```

#Clustering

Simply plotting the enire railroad model shows some interesting clusters. For example, the words "negligence, "injury," "damage," "jury," and "defendent" are all clustered together.
```{r}
plot(rr_model)
```


Similarity searching confirms the existence of this cluster.
```{r}
rr_model %>% nearest_to(rr_model[["injury"]])
```




